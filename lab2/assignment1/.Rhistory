library(glmnet)
library(dplyr)
#Read data from csv file
tecator = read.csv("tecator.csv", header = TRUE)
setwd("~/TDDE01/TDDE01-Labs/lab2/assignment1")
library(glmnet)
library(dplyr)
#Read data from csv file
tecator = read.csv("tecator.csv", header = TRUE)
#Spliting data 50%/50% between training and testing
n <- dim(tecator)[1]
set.seed(12345)
id <- sample(1:n, floor(n * 0.5))
trainingData <- tecator[id, ]
testData <- tecator[-id, ]
#Removing unessecary columns from the training & test data such as sample and others
trainingData = trainingData  %>% select(2:102)
testData = testData  %>% select(2:102)
##### Task 1 #####
#################
#Create a linear model
m1 = lm(Fat~., data=trainingData)
summary(m1) #displays the model coefficients
#make predictions on training and test data
pred_training = predict(m1, trainingData)
pred_test = predict(m1, testData)
#Calculate Mean Square Error, by comparing predicted values with real values
MSE_training = mean((trainingData$Fat - pred_training)^2)
MSE_test = mean((testData$Fat - pred_test)^2)
##### Task 2 #####
#################
#Cost function = RSS + lambda * sum (|beta|)
#cost = Mean((trainingData$Fat - pred_training)^2) + lambda * sum(abs(coef(m1)))
##### Task 3 #####
#################
x =as.matrix(trainingData %>% select(1:100))
y = as.matrix(trainingData$Fat)
m_lasso=cv.glmnet(x, y,family="gaussian", alpha=1)
plot(m_lasso, xvar = "lambda", label = TRUE)
# It looks from the plot that lambda can have values from 0.4 to 0.9
##### Task 4 #####
#################
m_ridge=cv.glmnet(x, y,family="gaussian", alpha=0)
plot(m_ridge, xvar = "lambda", label = TRUE)
##### Task 5 #####
#################
opt_lambda = m_lasso$lambda.min
cat("Optimal lambda:", opt_lambda, "\n")
library(glmnet)
library(dplyr)
#Read data from csv file
tecator = read.csv("tecator.csv", header = TRUE)
#Spliting data 50%/50% between training and testing
n <- dim(tecator)[1]
set.seed(12345)
id <- sample(1:n, floor(n * 0.5))
trainingData <- tecator[id, ]
testData <- tecator[-id, ]
#Removing unessecary columns from the training & test data such as sample and others
trainingData = trainingData  %>% select(2:102)
testData = testData  %>% select(2:102)
##### Task 1 #####
#################
#Create a linear model
m1 = lm(Fat~., data=trainingData)
summary(m1) #displays the model coefficients
#make predictions on training and test data
pred_training = predict(m1, trainingData)
pred_test = predict(m1, testData)
#Calculate Mean Square Error, by comparing predicted values with real values
MSE_training = mean((trainingData$Fat - pred_training)^2)
MSE_test = mean((testData$Fat - pred_test)^2)
##### Task 2 #####
#################
#Cost function = RSS + lambda * sum (|beta|)
#cost = Mean((trainingData$Fat - pred_training)^2) + lambda * sum(abs(coef(m1)))
##### Task 3 #####
#################
x =as.matrix(trainingData %>% select(1:100))
y = as.matrix(trainingData$Fat)
m_lasso=glmnet(x, y,family="gaussian", alpha=1)
plot(m_lasso, xvar = "lambda", label = TRUE)
# It looks from the plot that lambda can have values from 0.4 to 0.9
##### Task 4 #####
#################
m_ridge=cv.glmnet(x, y,family="gaussian", alpha=0)
plot(m_ridge, xvar = "lambda", label = TRUE)
##### Task 5 #####
#################
opt_lambda = m_lasso$lambda.min
cat("Optimal lambda:", opt_lambda, "\n")
library(glmnet)
library(dplyr)
#Read data from csv file
tecator = read.csv("tecator.csv", header = TRUE)
#Spliting data 50%/50% between training and testing
n <- dim(tecator)[1]
set.seed(12345)
id <- sample(1:n, floor(n * 0.5))
trainingData <- tecator[id, ]
testData <- tecator[-id, ]
#Removing unessecary columns from the training & test data such as sample and others
trainingData = trainingData  %>% select(2:102)
testData = testData  %>% select(2:102)
##### Task 1 #####
#################
#Create a linear model
m1 = lm(Fat~., data=trainingData)
summary(m1) #displays the model coefficients
#make predictions on training and test data
pred_training = predict(m1, trainingData)
pred_test = predict(m1, testData)
#Calculate Mean Square Error, by comparing predicted values with real values
MSE_training = mean((trainingData$Fat - pred_training)^2)
MSE_test = mean((testData$Fat - pred_test)^2)
##### Task 2 #####
#################
#Cost function = RSS + lambda * sum (|beta|)
#cost = Mean((trainingData$Fat - pred_training)^2) + lambda * sum(abs(coef(m1)))
##### Task 3 #####
#################
x =as.matrix(trainingData %>% select(1:100))
y = as.matrix(trainingData$Fat)
m_lasso=glmnet(x, y,family="gaussian", alpha=1)
plot(m_lasso, xvar = "lambda", label = TRUE)
# It looks from the plot that lambda can have values from 0.4 to 0.9
##### Task 4 #####
#################
m_ridge=glmnet(x, y,family="gaussian", alpha=0)
plot(m_ridge, xvar = "lambda", label = TRUE)
##### Task 5 #####
#################
opt_lambda = m_lasso$lambda.min
cat("Optimal lambda:", opt_lambda, "\n")
m_cv_lasso = cv.lasso(x, y, family='gaussian', alpha=1)
m_cv_lasso = cv.glmnet(x, y, family='gaussian', alpha=1)
plot(m_cv_lasso, xvar = "lambda", label = TRUE)
opt_lambda = m_cv_lasso$lambda.min
cat("Optimal lambda:", opt_lambda, "\n")
optimal_lasso = glmnet(x,y,family='gaussian', alpha = 1, lambda = opt_lambda)
plot(optimal_lasso)
